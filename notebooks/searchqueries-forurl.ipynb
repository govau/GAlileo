{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query analysis by URL\n",
    "\n",
    "Find typical queries for a series of URLs from the Google Search Console API reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import snowballstemmer #  pip3 install snowballstemmer\n",
    "stemmer = snowballstemmer.stemmer('English')\n",
    "australian_re = re.compile(\"australian?\")\n",
    "def check_unordered_string(string, strings):\n",
    "    if string in strings:\n",
    "        return False\n",
    "    string_set = set(australian_re.sub('',string).strip().split(' '))\n",
    "    #print(string_set)\n",
    "    for other_string in strings:\n",
    "        if string_set == set(australian_re.sub('',other_string).strip().split(' ')):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def stem_dedup(strings):\n",
    "    new_strings = {}\n",
    "    for string in strings:\n",
    "        new_string = ' '.join(stemmer.stemWord(word)\n",
    "                    for word in string.split(' '))\n",
    "        #print(string,' => ',new_string)\n",
    "        if new_string not in new_strings and check_unordered_string(string, new_strings.values()):\n",
    "            new_strings[new_string] = string\n",
    "    return new_strings.values()\n",
    "def find_modal_substring(strings):\n",
    "    from functools import partial, reduce\n",
    "    from itertools import chain\n",
    "    from typing import Iterator\n",
    "    \n",
    "\n",
    "    def ngram(seq: str, n: int) -> Iterator[str]:\n",
    "        return (seq[i: i+n] for i in range(0, len(seq)-n+1))\n",
    "\n",
    "    def allngram(seq: str, minn=1, maxn=None) -> Iterator[str]:\n",
    "        lengths = range(minn, maxn) if maxn else range(minn, len(seq))\n",
    "        ngrams = map(partial(ngram, seq), lengths)\n",
    "        return set(chain.from_iterable(ngrams))\n",
    "    \n",
    "    seqs_ngrams = map(partial(allngram), strings)\n",
    "    counts = Counter(chain.from_iterable(seqs_ngrams))\n",
    "    large_counts = {}\n",
    "    for sstr in counts:\n",
    "        key = counts[sstr]*len(sstr)\n",
    "        if len(sstr) > len(large_counts.get(key,\"\")):\n",
    "            large_counts[key] = sstr\n",
    "    largest_counts = dict(sorted(large_counts.items(),reverse=True))\n",
    "\n",
    "    modal_ngram = max(list(largest_counts.values())[:5], key=len).strip()\n",
    "    modal_words_search = re.search(r\"\\b.?\"+re.escape(modal_ngram)+r\".?\\b\",'\\n'.join(strings))\n",
    "    modal_words = modal_words_search.group(0).strip() if modal_words_search else None\n",
    "    if modal_words and modal_words.startswith(\".\"):\n",
    "        modal_words = modal_words[1:]\n",
    "    return modal_words or modal_ngram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tablib import Dataset\n",
    "# query\tpage\tclicks\timpressions\tclick_thru_ratio\tsearch_result_position\n",
    "imported_data = Dataset().load(open('data_searchqueries_australiagovau_websearch_20190916_20190923.csv').read())\n",
    "queries = set()\n",
    "queries_by_page = {}\n",
    "query_clicks = {}\n",
    "query_impressions = {}\n",
    "for row in imported_data.dict:\n",
    "    query =  row['query'].strip()\n",
    "    if ('visa' in row['query'] or 'migration' in row['query'] or 'visa' in row['page']) and query != '':\n",
    "        queries.add(query)\n",
    "        if row['page'] not in queries_by_page:\n",
    "                queries_by_page[row['page']] = set()\n",
    "        queries_by_page[row['page']].add(query)\n",
    "        query_clicks[row['query']] = float(row['clicks'])\n",
    "        query_impressions[row['query']] = float(row['impressions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for page,queries in queries_by_page.items():\n",
    "    if len(queries) < 75:\n",
    "        total_clicks = 0\n",
    "        total_impressions = 0\n",
    "        for query in queries:\n",
    "            total_clicks += query_clicks[query]\n",
    "            total_impressions += query_impressions[query]\n",
    "        if total_clicks > 10:\n",
    "            print(page)\n",
    "            print(len(queries))\n",
    "            for query in queries:    \n",
    "                if query_clicks[query] > 10:\n",
    "                    print(query)\n",
    "                    print ('clicks: ', query_clicks[query])\n",
    "                elif query_impressions[query]> total_impressions* 0.05:\n",
    "                    print(query)\n",
    "                    print ('impressions: ', query_impressions[query])\n",
    "                #else:\n",
    "                #    print ('too small')\n",
    "            print(' ')\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tablib\n",
    "result = tablib.Dataset()\n",
    "result.headers = ['page','query','hits','impressions']\n",
    "for page,queries in queries_by_page.items():\n",
    "    if len(queries) > 75:\n",
    "        total_clicks = 0\n",
    "        total_impressions = 0\n",
    "        for query in queries:\n",
    "            total_clicks += query_clicks[query]\n",
    "            total_impressions += query_impressions[query]\n",
    "        if total_clicks > 10:\n",
    "            \n",
    "            print(page)\n",
    "            #print(total_clicks, total_impressions)\n",
    "            #print(len(queries))\n",
    "            clean_queries = []\n",
    "            for query in queries:    \n",
    "                if query_clicks[query]> total_clicks* 0.01 and query_clicks[query]> 7:\n",
    "                    clean_queries.append(query)\n",
    "                    #print(query)\n",
    "                    #print ('clicks: ', query_clicks[query])\n",
    "                elif query_impressions[query]> total_impressions* 0.1:\n",
    "                    clean_queries.append(query)\n",
    "                    #print(query)\n",
    "                    #print ('impressions: ', query_impressions[query])\n",
    "                #else:\n",
    "                #    print ('too small')\n",
    "            clean_queries = set([x.replace('immigrate to','migrate to') for x in clean_queries])\n",
    "            #print(clean_queries)\n",
    "            dedup_queries = (stem_dedup(clean_queries))\n",
    "            print(dedup_queries)\n",
    "            for query in dedup_queries:\n",
    "                #print(query, query_clicks[query], query_impressions[query])\n",
    "                result.append([page,query,query_clicks[query], query_impressions[query]])\n",
    "            #print(find_modal_substring(clean_queries))\n",
    "            print(' ')\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
